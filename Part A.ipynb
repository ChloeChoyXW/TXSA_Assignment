{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f06c6bec",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b694153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Textual information in the world can be broadly categorized into two main types: facts and opinions. Facts are objective expressions about entities, events, and their properties. Opinions are usually subjective expressions that describe people’s sentiments, appraisals, or feelings toward entities, events, and their properties.\n"
     ]
    }
   ],
   "source": [
    "# Read the text in Data_1 as a string\n",
    "with open('Part A Text Data/Data_1.txt', 'r') as file: \n",
    "    data = file.read()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e704a8",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "306cef22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NLTK:  56 tokens\n",
      "['Textual', 'information', 'in', 'the', 'world', 'can', 'be', 'broadly', 'categorized', 'into', 'two', 'main', 'types', ':', 'facts', 'and', 'opinions', '.', 'Facts', 'are', 'objective', 'expressions', 'about', 'entities', ',', 'events', ',', 'and', 'their', 'properties', '.', 'Opinions', 'are', 'usually', 'subjective', 'expressions', 'that', 'describe', 'people', '’', 's', 'sentiments', ',', 'appraisals', ',', 'or', 'feelings', 'toward', 'entities', ',', 'events', ',', 'and', 'their', 'properties', '.']\n",
      "\n",
      "Using Split Function:  44 tokens\n",
      "['Textual', 'information', 'in', 'the', 'world', 'can', 'be', 'broadly', 'categorized', 'into', 'two', 'main', 'types:', 'facts', 'and', 'opinions.', 'Facts', 'are', 'objective', 'expressions', 'about', 'entities,', 'events,', 'and', 'their', 'properties.', 'Opinions', 'are', 'usually', 'subjective', 'expressions', 'that', 'describe', 'people’s', 'sentiments,', 'appraisals,', 'or', 'feelings', 'toward', 'entities,', 'events,', 'and', 'their', 'properties.']\n",
      "\n",
      "Using Regular Expression:  45 tokens\n",
      "['Textual', 'information', 'in', 'the', 'world', 'can', 'be', 'broadly', 'categorized', 'into', 'two', 'main', 'types', 'facts', 'and', 'opinions', 'Facts', 'are', 'objective', 'expressions', 'about', 'entities', 'events', 'and', 'their', 'properties', 'Opinions', 'are', 'usually', 'subjective', 'expressions', 'that', 'describe', 'people', 's', 'sentiments', 'appraisals', 'or', 'feelings', 'toward', 'entities', 'events', 'and', 'their', 'properties']\n"
     ]
    }
   ],
   "source": [
    "# Using NLTK packages\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk_tokens = word_tokenize(data)\n",
    "print(\"Using NLTK: \", len(nltk_tokens), \"tokens\")\n",
    "print(nltk_tokens)\n",
    "\n",
    "# Using split function\n",
    "split_tokens = data.split()  # By default, split string into a list of words by whitespace\n",
    "print(\"\\nUsing Split Function: \", len(split_tokens), \"tokens\")\n",
    "print(split_tokens)\n",
    "\n",
    "# Using regular expression \n",
    "import re\n",
    "regex_tokens = re.findall(r'\\b\\w+\\b', data)\n",
    "print(\"\\nUsing Regular Expression: \", len(regex_tokens), \"tokens\")\n",
    "print(regex_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63964b28",
   "metadata": {},
   "source": [
    "Stop Words and Punctuation Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92e2e503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing stop words and punctuation:  29  tokens\n",
      "['textual', 'information', 'world', 'broadly', 'categorized', 'two', 'main', 'types', 'facts', 'opinions', 'facts', 'objective', 'expressions', 'entities', 'events', 'properties', 'opinions', 'usually', 'subjective', 'expressions', 'describe', 'people', 'sentiments', 'appraisals', 'feelings', 'toward', 'entities', 'events', 'properties'] \n",
      "\n",
      "26  stop words found:\n",
      "['in', 'the', 'can', 'be', 'into', ':', 'and', '.', 'are', 'about', ',', ',', 'and', 'their', '.', 'are', 'that', 's', ',', ',', 'or', ',', ',', 'and', 'their', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk, string\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "content_lower_case = data.lower()  # Convert all words to lower case before removing stopwords and punctuation\n",
    "tokens = nltk.tokenize.word_tokenize(content_lower_case)\n",
    "\n",
    "stop_tokens = nltk.corpus.stopwords.words(\"english\") + list(string.punctuation)\n",
    "filtered_tokens = []\n",
    "stop_words = []\n",
    "\n",
    "for word in tokens:\n",
    "    if  word in stop_tokens:\n",
    "        stop_words.append(word)\n",
    "    elif len(word) > 1:\n",
    "        filtered_tokens.append(word)\n",
    "\n",
    "print(\"After removing stop words and punctuation: \",len(filtered_tokens),\" tokens\")\n",
    "print(filtered_tokens,\"\\n\")\n",
    "print(len(stop_words),\" stop words found:\")\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e025727",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1f9314",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b35ee03",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc3fd37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab035709",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcd0678",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
